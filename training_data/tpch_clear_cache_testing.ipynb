{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d531d42-6c59-494d-8f95-f0b1ed21054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoped/spark-autotuner/training_data\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import time \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType, LongType, StringType, StructField, StructType)\n",
    "\n",
    "import platform,socket,re,uuid,json,psutil,logging\n",
    "\n",
    "\n",
    "# Schemas for all table types here. These should be in separate scripts when\n",
    "# refactoring code.\n",
    "CUSTOMER_SCHEMA = StructType([\n",
    "    StructField(\"c_custkey\", LongType()),\n",
    "    StructField(\"c_name\", StringType()),\n",
    "    StructField(\"c_address\", StringType()),\n",
    "    StructField(\"c_nationkey\", LongType()),\n",
    "    StructField(\"c_phone\", StringType()),\n",
    "    StructField(\"c_acctbal\", DoubleType()),\n",
    "    StructField(\"c_mktsegment\", StringType()),\n",
    "    StructField(\"c_comment\", StringType()),\n",
    "])\n",
    "\n",
    "LINEITEM_SCHEMA = StructType([\n",
    "    StructField(\"l_orderkey\", LongType()),  \n",
    "    StructField(\"l_partkey\", LongType()),\n",
    "    StructField(\"l_suppkey\", LongType()),\n",
    "    StructField(\"l_linenumber\", LongType()),\n",
    "    StructField(\"l_quantity\", DoubleType()),\n",
    "    StructField(\"l_extendedprice\", DoubleType()),\n",
    "    StructField(\"l_discount\", DoubleType()),\n",
    "    StructField(\"l_tax\", DoubleType()),\n",
    "    StructField(\"l_returnflag\", StringType()),\n",
    "    StructField(\"l_linestatus\", StringType()),\n",
    "    StructField(\"l_shipdate\", StringType()),\n",
    "    StructField(\"l_commitdate\", StringType()),\n",
    "    StructField(\"l_receiptdate\", StringType()),\n",
    "    StructField(\"l_shipinstruct\", StringType()),\n",
    "    StructField(\"l_shipmode\", StringType()),\n",
    "    StructField(\"l_comment\", StringType())\n",
    "])\n",
    "\n",
    "NATION_SCHEMA = StructType([\n",
    "    StructField(\"n_nationkey\", LongType()), \n",
    "    StructField(\"n_name\", StringType()),\n",
    "    StructField(\"n_regionkey\", LongType()),\n",
    "    StructField(\"n_comment\", StringType()),\n",
    "])\n",
    "\n",
    "ORDER_SCHEMA = StructType([\n",
    "    StructField(\"o_orderkey\", LongType()),\n",
    "    StructField(\"o_custkey\", LongType()),\n",
    "    StructField(\"o_orderstatus\", StringType()),\n",
    "    StructField(\"o_totalprice\", DoubleType()),\n",
    "    StructField(\"o_orderdate\", StringType()),\n",
    "    StructField(\"o_orderpriority\", StringType()),\n",
    "    StructField(\"o_clerk\", StringType()),\n",
    "    StructField(\"o_shippriority\", LongType()),\n",
    "    StructField(\"o_comment\", StringType())\n",
    "])\n",
    "\n",
    "PART_SCHEMA = StructType([\n",
    "    StructField(\"p_partkey\", LongType()),    \n",
    "    StructField(\"p_name\", StringType()),\n",
    "    StructField(\"p_mfgr\", StringType()),\n",
    "    StructField(\"p_brand\", StringType()),\n",
    "    StructField(\"p_type\", StringType()),\n",
    "    StructField(\"p_size\", LongType()),\n",
    "    StructField(\"p_container\", StringType()),\n",
    "    StructField(\"p_retailprice\", DoubleType()),\n",
    "    StructField(\"p_comment\", StringType()),\n",
    "])\n",
    "\n",
    "PARTSUPP_SCHEMA = StructType([\n",
    "    StructField(\"ps_partkey\", LongType()),\n",
    "    StructField(\"ps_suppkey\", LongType()),\n",
    "    StructField(\"ps_availqty\", LongType()),\n",
    "    StructField(\"ps_supplycost\", DoubleType()),\n",
    "    StructField(\"ps_comment\", StringType())\n",
    "])\n",
    "\n",
    "REGION_SCHEMA = StructType([\n",
    "    StructField(\"r_regionkey\", LongType()),   \n",
    "    StructField(\"r_name\", StringType()),\n",
    "    StructField(\"r_comment\", StringType()),  \n",
    "])\n",
    "\n",
    "SUPPLIER_SCHEMA = StructType([\n",
    "    StructField(\"s_suppkey\", LongType()),    \n",
    "    StructField(\"s_name\", StringType()),\n",
    "    StructField(\"s_address\", StringType()),\n",
    "    StructField(\"s_nationkey\", LongType()),\n",
    "    StructField(\"s_phone\", StringType()),\n",
    "    StructField(\"s_acctbal\", DoubleType()),\n",
    "    StructField(\"s_comment\", StringType())\n",
    "])\n",
    "\n",
    "TABLE_SCHEMA_MAP = {\n",
    "        \"customer\": CUSTOMER_SCHEMA,\n",
    "        \"lineitem\": LINEITEM_SCHEMA,\n",
    "        \"nation\": NATION_SCHEMA,\n",
    "        \"region\": REGION_SCHEMA,\n",
    "        \"orders\": ORDER_SCHEMA,\n",
    "        \"part\": PART_SCHEMA,\n",
    "        \"partsupp\": PARTSUPP_SCHEMA,\n",
    "        \"supplier\": SUPPLIER_SCHEMA,\n",
    "}\n",
    "\n",
    "CURRENT_FILE_PATH = os.path.dirname(os.getcwd())\n",
    "if \"training_data\" not in CURRENT_FILE_PATH:\n",
    "    CURRENT_FILE_PATH += \"/training_data\"\n",
    "print(CURRENT_FILE_PATH)\n",
    "    \n",
    "def getSystemInfo():\n",
    "    info={}\n",
    "    try:\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['num_cpus'] = os.cpu_count()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        info['total_storage']=str(round(psutil.disk_usage(CURRENT_FILE_PATH).total / (1024.0 **3)))+\" GB\"\n",
    "        info['free_storage']=str(round(psutil.disk_usage(CURRENT_FILE_PATH).free / (1024.0 **3)))+\" GB\"\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "    return info\n",
    "\n",
    "# read in 22 TPCH queries\n",
    "TPCH_QUERIES = {}\n",
    "for i in range(1, 23):\n",
    "    with open(f\"{CURRENT_FILE_PATH}/queries/{i}.sql\") as f:\n",
    "        TPCH_QUERIES[i] = f.read() \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0c46ed8-e3d0-4851-ad65-52338c1dfb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries_og(parameters, n=10, debug=False, find_median_runtime=True):\n",
    "    '''\n",
    "    Run TPC-H queries 10 times and take the median runtime of each query \n",
    "    to generate a single training run for a set of parameters.\n",
    "    \n",
    "    Input: \n",
    "    parameters: list of parameter dictionaries \n",
    "    debug: if true will print out params and result time, false suppresses print statements\n",
    "    \n",
    "    Returns: \n",
    "    training_data dictionary with params and results \n",
    "    '''\n",
    "    result = {'params': [p.copy() for p in parameters], 'runtimes': {'total': []}}\n",
    "    spark = None\n",
    "    # add chosen parameter values to spark\n",
    "    param_name_index = {}\n",
    "    try:\n",
    "        conf = SparkConf(loadDefaults=False)\n",
    "        spark_params = []\n",
    "        for i, param in enumerate(parameters):\n",
    "            if param['spark_param']:\n",
    "                spark_params.append((param['name'], str(param['cur_value'])))\n",
    "                param_name_index[param['name']] = i\n",
    "\n",
    "        conf.setAll(spark_params)\n",
    "        spark  = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "                \n",
    "    except Exception as e:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        # this might happen because some parameters are related,\n",
    "        # and we might have made an impossible parameter assignment\n",
    "        result = {'params':parameters, 'runtimes': {}, 'msg': str(e)}\n",
    "        if debug:\n",
    "            print(\"error when setting \", parameters, e)\n",
    "        return result\n",
    "    \n",
    "    configurations = spark.sparkContext.getConf().getAll()\n",
    "    if debug:\n",
    "        print(\"Configuration\")\n",
    "    for item in configurations: \n",
    "        if debug:\n",
    "            print(item)\n",
    "        if param_name_index.get(item[0]) is not None:\n",
    "            assert item[1] == param[param_name_index.get(item[0])]['cur_value'], f'Spark session param {item} != {param[param_name_index.get(item[0])]}'\n",
    "    \n",
    "    # load tables\n",
    "    if debug:\n",
    "        print(\"loading tables\")\n",
    "    tables = {}\n",
    "    for table_name, table_schema in TABLE_SCHEMA_MAP.items():\n",
    "        table = spark.read.csv(f\"{CURRENT_FILE_PATH}/{SF_STR}/{table_name}.tbl\", sep = \"|\",\n",
    "                               schema=table_schema)\n",
    "        table.createOrReplaceTempView(table_name)\n",
    "        tables[table_name] = table\n",
    "    \n",
    "    if debug:\n",
    "        print(\"running queries\")\n",
    "    # take median of n runs for each query\n",
    "    for j in range(n):\n",
    "        result['runtimes']['total'].append(0)\n",
    "        for qnum, qtext in TPCH_QUERIES.items(): \n",
    "                # Measure execution time of sql query.\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                results = spark.sql(qtext, **tables)\n",
    "                end_time = time.time()\n",
    "                query_time = end_time - start_time\n",
    "                result['runtimes'].setdefault(qnum, []).append(query_time)\n",
    "                result['runtimes']['total'][-1] += query_time\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(f\"failed while running query {qnum}...  \")\n",
    "    if debug:\n",
    "        print(\"done running queries\")\n",
    "    if find_median_runtime:\n",
    "        # take median of all runtimes as final output\n",
    "        for key, times in result['runtimes'].items():\n",
    "            result['runtimes'][key] = np.median(times)\n",
    "            if debug:\n",
    "                print(key, result['runtimes'][key])\n",
    "    # reset spark so we can load new param config next time\n",
    "    spark.stop()\n",
    "    #spark.newSession()#_instantiatedContext attribute of the session to None after calling session.stop().\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44e7f15a-467b-423a-898a-63ba81fd7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1 # GB, default table scale factor\n",
    "job_name = 'local_run'\n",
    "SF_STR = f\"sf{sf}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f42b3d5c-e1ae-4f87-8828-1b8eb4c37180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2102336883544922,\n",
       "  0.19797110557556152,\n",
       "  0.18102312088012695,\n",
       "  0.17831206321716309,\n",
       "  0.18066120147705078,\n",
       "  0.22466421127319336,\n",
       "  0.18740582466125488,\n",
       "  0.17984366416931152,\n",
       "  0.1779019832611084,\n",
       "  0.17885327339172363,\n",
       "  0.17812252044677734,\n",
       "  0.2745347023010254,\n",
       "  0.18263888359069824,\n",
       "  0.2833106517791748,\n",
       "  0.1967933177947998,\n",
       "  0.21156930923461914,\n",
       "  0.23160982131958008,\n",
       "  0.2412569522857666,\n",
       "  0.17873525619506836,\n",
       "  0.17847228050231934,\n",
       "  0.17816162109375,\n",
       "  0.18703699111938477,\n",
       "  0.1779642105102539,\n",
       "  0.20836806297302246,\n",
       "  0.1785109043121338,\n",
       "  0.17832612991333008,\n",
       "  0.1867237091064453,\n",
       "  0.17916083335876465,\n",
       "  0.18141889572143555,\n",
       "  0.296567440032959,\n",
       "  0.17829656600952148,\n",
       "  0.21231412887573242,\n",
       "  0.24968457221984863,\n",
       "  0.18040180206298828,\n",
       "  0.17844295501708984,\n",
       "  0.20157527923583984,\n",
       "  0.18844246864318848,\n",
       "  0.17975687980651855,\n",
       "  0.1783885955810547,\n",
       "  0.17870235443115234,\n",
       "  0.1829688549041748,\n",
       "  0.3233530521392822,\n",
       "  0.22744369506835938,\n",
       "  0.19604873657226562,\n",
       "  0.17841196060180664,\n",
       "  0.17839598655700684,\n",
       "  0.18747401237487793,\n",
       "  0.1873929500579834,\n",
       "  0.17843341827392578,\n",
       "  0.17820215225219727,\n",
       "  0.17756438255310059,\n",
       "  0.17813515663146973,\n",
       "  0.23815631866455078,\n",
       "  0.20382118225097656,\n",
       "  0.20121335983276367,\n",
       "  0.22371554374694824,\n",
       "  0.197953462600708,\n",
       "  0.1853485107421875,\n",
       "  0.2116260528564453,\n",
       "  0.2507796287536621,\n",
       "  0.21088528633117676,\n",
       "  0.21127057075500488,\n",
       "  0.25276899337768555,\n",
       "  0.20357060432434082,\n",
       "  0.1781446933746338,\n",
       "  0.21275663375854492,\n",
       "  0.18611502647399902,\n",
       "  0.17792153358459473,\n",
       "  0.17795920372009277,\n",
       "  0.17923521995544434,\n",
       "  0.17857146263122559,\n",
       "  0.23131918907165527,\n",
       "  0.24033904075622559,\n",
       "  0.17813348770141602,\n",
       "  0.1785264015197754,\n",
       "  0.17887377738952637,\n",
       "  0.1798551082611084,\n",
       "  0.25448060035705566,\n",
       "  0.20973682403564453,\n",
       "  0.18233871459960938,\n",
       "  0.17833471298217773,\n",
       "  0.17884564399719238,\n",
       "  0.252197265625,\n",
       "  0.1783285140991211,\n",
       "  0.24756646156311035,\n",
       "  0.1785902976989746,\n",
       "  0.17875909805297852,\n",
       "  0.2328176498413086,\n",
       "  0.23949027061462402,\n",
       "  0.17798542976379395,\n",
       "  0.2536652088165283,\n",
       "  0.18802714347839355,\n",
       "  0.17810297012329102,\n",
       "  0.1784498691558838,\n",
       "  0.17785120010375977,\n",
       "  0.18049335479736328,\n",
       "  0.24477839469909668,\n",
       "  0.22585535049438477,\n",
       "  0.19809436798095703,\n",
       "  0.26677417755126953],\n",
       " 0.18573176860809326,\n",
       " 0.3233530521392822)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_queries_og([], n=100, find_median_runtime=False)\n",
    "result['runtimes']['total'], np.median(result['runtimes']['total']), max(result['runtimes']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67529851-6fd6-4be5-b865-96be368fa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(parameters, n=10, debug=False, find_median_runtime=True):\n",
    "    '''\n",
    "    Run TPC-H queries 10 times and take the median runtime of each query \n",
    "    to generate a single training run for a set of parameters.\n",
    "    \n",
    "    Input: \n",
    "    parameters: list of parameter dictionaries \n",
    "    debug: if true will print out params and result time, false suppresses print statements\n",
    "    \n",
    "    Returns: \n",
    "    training_data dictionary with params and results \n",
    "    '''\n",
    "    result = {'params': [p.copy() for p in parameters], 'runtimes': {'total': []}}\n",
    "    spark = None\n",
    "    # add chosen parameter values to spark\n",
    "    param_name_index = {}\n",
    "    try:\n",
    "        conf = SparkConf(loadDefaults=False)\n",
    "        spark_params = []\n",
    "        for i, param in enumerate(parameters):\n",
    "            if param['spark_param']:\n",
    "                spark_params.append((param['name'], str(param['cur_value'])))\n",
    "                param_name_index[param['name']] = i\n",
    "\n",
    "        conf.setAll(spark_params)\n",
    "        spark  = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "        spark.catalog.clearCache() # clear cache\n",
    "                \n",
    "    except Exception as e:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        # this might happen because some parameters are related,\n",
    "        # and we might have made an impossible parameter assignment\n",
    "        result = {'params':parameters, 'runtimes': {}, 'msg': str(e)}\n",
    "        if debug:\n",
    "            print(\"error when setting \", parameters, e)\n",
    "        return result\n",
    "    \n",
    "    configurations = spark.sparkContext.getConf().getAll()\n",
    "    if debug:\n",
    "        print(\"Configuration\")\n",
    "    for item in configurations: \n",
    "        if debug:\n",
    "            print(item)\n",
    "        if param_name_index.get(item[0]) is not None:\n",
    "            assert item[1] == param[param_name_index.get(item[0])]['cur_value'], f'Spark session param {item} != {param[param_name_index.get(item[0])]}'\n",
    "    \n",
    "    # load tables\n",
    "    if debug:\n",
    "        print(\"loading tables\")\n",
    "    tables = {}\n",
    "    for table_name, table_schema in TABLE_SCHEMA_MAP.items():\n",
    "        table = spark.read.csv(f\"{CURRENT_FILE_PATH}/{SF_STR}/{table_name}.tbl\", sep = \"|\",\n",
    "                               schema=table_schema)\n",
    "        table.createOrReplaceTempView(table_name)\n",
    "        tables[table_name] = table\n",
    "    \n",
    "    if debug:\n",
    "        print(\"running queries\")\n",
    "        \n",
    "    # take median of n runs for each query\n",
    "    for j in range(n):\n",
    "        spark.catalog.clearCache() # clear cache before each run\n",
    "        result['runtimes']['total'].append(0)\n",
    "        for qnum, qtext in TPCH_QUERIES.items(): \n",
    "                # Measure execution time of sql query.\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                results = spark.sql(qtext, **tables)\n",
    "                end_time = time.time()\n",
    "                query_time = end_time - start_time\n",
    "                result['runtimes'].setdefault(qnum, []).append(query_time)\n",
    "                result['runtimes']['total'][-1] += query_time\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(f\"failed while running query {qnum}...  \")\n",
    "    if debug:\n",
    "        print(\"done running queries\")\n",
    "    if find_median_runtime:\n",
    "        # take median of all runtimes as final output\n",
    "        for key, times in result['runtimes'].items():\n",
    "            result['runtimes'][key] = np.median(times)\n",
    "            if debug:\n",
    "                print(key, result['runtimes'][key])\n",
    "    # reset spark so we can load new param config next time\n",
    "    spark.catalog.clearCache() # clear cache at the end of each run just in case?\n",
    "    spark.stop()\n",
    "    #spark.newSession()#_instantiatedContext attribute of the session to None after calling session.stop().\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8726ab3-5211-46e8-97ad-7a5bb0b7df30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2587094306945801,\n",
       "  0.20500421524047852,\n",
       "  0.17894196510314941,\n",
       "  0.17899274826049805,\n",
       "  0.18035173416137695,\n",
       "  0.23854613304138184,\n",
       "  0.17979025840759277,\n",
       "  0.17872023582458496,\n",
       "  0.17866897583007812,\n",
       "  0.17903900146484375,\n",
       "  0.22313928604125977,\n",
       "  0.2436084747314453,\n",
       "  0.17853474617004395,\n",
       "  0.23714065551757812,\n",
       "  0.23153400421142578,\n",
       "  0.2521653175354004,\n",
       "  0.24042224884033203,\n",
       "  0.270127534866333,\n",
       "  0.17946720123291016,\n",
       "  0.18448901176452637,\n",
       "  0.17935895919799805,\n",
       "  0.1782224178314209,\n",
       "  0.23639488220214844,\n",
       "  0.24494051933288574,\n",
       "  0.18637585639953613,\n",
       "  0.17913818359375,\n",
       "  0.17864012718200684,\n",
       "  0.17867469787597656,\n",
       "  0.178422212600708,\n",
       "  0.21886754035949707,\n",
       "  0.18841195106506348,\n",
       "  0.1786661148071289,\n",
       "  0.17829394340515137,\n",
       "  0.1783301830291748,\n",
       "  0.25531625747680664,\n",
       "  0.2540466785430908,\n",
       "  0.23046302795410156,\n",
       "  0.17897796630859375,\n",
       "  0.1786348819732666,\n",
       "  0.20357036590576172,\n",
       "  0.21613764762878418,\n",
       "  0.20429015159606934,\n",
       "  0.22131133079528809,\n",
       "  0.18821978569030762,\n",
       "  0.21754693984985352,\n",
       "  0.2677419185638428,\n",
       "  0.17824506759643555,\n",
       "  0.2125246524810791,\n",
       "  0.17835259437561035,\n",
       "  0.18591046333312988,\n",
       "  0.1945810317993164,\n",
       "  0.17888593673706055,\n",
       "  0.17849087715148926,\n",
       "  0.20780515670776367,\n",
       "  0.2139875888824463,\n",
       "  0.20754218101501465,\n",
       "  0.2234477996826172,\n",
       "  0.1785738468170166,\n",
       "  0.2314314842224121,\n",
       "  0.2653930187225342,\n",
       "  0.17871451377868652,\n",
       "  0.1778724193572998,\n",
       "  0.17856931686401367,\n",
       "  0.17794537544250488,\n",
       "  0.18463921546936035,\n",
       "  0.20179104804992676,\n",
       "  0.17822551727294922,\n",
       "  0.1780388355255127,\n",
       "  0.17841196060180664,\n",
       "  0.17815613746643066,\n",
       "  0.19690775871276855,\n",
       "  0.24492835998535156,\n",
       "  0.2248978614807129,\n",
       "  0.18031883239746094,\n",
       "  0.17824459075927734,\n",
       "  0.18628525733947754,\n",
       "  0.2302112579345703,\n",
       "  0.23190093040466309,\n",
       "  0.18151617050170898,\n",
       "  0.17823243141174316,\n",
       "  0.2713127136230469,\n",
       "  0.17798852920532227,\n",
       "  0.17993617057800293,\n",
       "  0.25158262252807617,\n",
       "  0.18635201454162598,\n",
       "  0.1783146858215332,\n",
       "  0.17784833908081055,\n",
       "  0.17829251289367676,\n",
       "  0.18169450759887695,\n",
       "  0.21005010604858398,\n",
       "  0.28049230575561523,\n",
       "  0.17938733100891113,\n",
       "  0.1802675724029541,\n",
       "  0.178297758102417,\n",
       "  0.17842483520507812,\n",
       "  0.2537243366241455,\n",
       "  0.1946873664855957,\n",
       "  0.17944955825805664,\n",
       "  0.224900484085083,\n",
       "  0.3047804832458496],\n",
       " 0.18527483940124512,\n",
       " 0.3047804832458496)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_queries([], n=100, find_median_runtime=False)\n",
    "result['runtimes']['total'], np.median(result['runtimes']['total']), max(result['runtimes']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0377e-4492-47e5-a0b2-47b819802d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combo = []\n",
    "for i in range(20):\n",
    "    result = run_queries_og([], n=20, find_median_runtime=False)\n",
    "    results_combo.append(result['runtimes']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e6894-0ac3-4f09-b36f-251f72586797",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_times = [r[0] for r in results_combo]\n",
    "all_times = []\n",
    "for r in results_combo:\n",
    "    all_times += r\n",
    "\n",
    "def get_stats(runtimes):\n",
    "    print(f'median: {round(np.median(runtimes), 5)}, average: {round(np.average(runtimes), 5)}, std: {round(np.std(runtimes), 5)}, min: {round(min(runtimes), 5)}, max: {round(max(runtimes), 5)}')\n",
    "\n",
    "print(get_stats(first_times))\n",
    "print(get_stats(all_times))\n",
    "print('---')\n",
    "for r in results_combo:\n",
    "    print(get_stats(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50ba3c-f2f4-4c4d-9826-0ebe71957ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
