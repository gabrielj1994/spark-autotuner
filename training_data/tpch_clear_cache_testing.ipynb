{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d531d42-6c59-494d-8f95-f0b1ed21054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoped/spark-autotuner/training_data\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import time \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType, LongType, StringType, StructField, StructType)\n",
    "\n",
    "import platform,socket,re,uuid,json,psutil,logging\n",
    "\n",
    "\n",
    "# Schemas for all table types here. These should be in separate scripts when\n",
    "# refactoring code.\n",
    "CUSTOMER_SCHEMA = StructType([\n",
    "    StructField(\"c_custkey\", LongType()),\n",
    "    StructField(\"c_name\", StringType()),\n",
    "    StructField(\"c_address\", StringType()),\n",
    "    StructField(\"c_nationkey\", LongType()),\n",
    "    StructField(\"c_phone\", StringType()),\n",
    "    StructField(\"c_acctbal\", DoubleType()),\n",
    "    StructField(\"c_mktsegment\", StringType()),\n",
    "    StructField(\"c_comment\", StringType()),\n",
    "])\n",
    "\n",
    "LINEITEM_SCHEMA = StructType([\n",
    "    StructField(\"l_orderkey\", LongType()),  \n",
    "    StructField(\"l_partkey\", LongType()),\n",
    "    StructField(\"l_suppkey\", LongType()),\n",
    "    StructField(\"l_linenumber\", LongType()),\n",
    "    StructField(\"l_quantity\", DoubleType()),\n",
    "    StructField(\"l_extendedprice\", DoubleType()),\n",
    "    StructField(\"l_discount\", DoubleType()),\n",
    "    StructField(\"l_tax\", DoubleType()),\n",
    "    StructField(\"l_returnflag\", StringType()),\n",
    "    StructField(\"l_linestatus\", StringType()),\n",
    "    StructField(\"l_shipdate\", StringType()),\n",
    "    StructField(\"l_commitdate\", StringType()),\n",
    "    StructField(\"l_receiptdate\", StringType()),\n",
    "    StructField(\"l_shipinstruct\", StringType()),\n",
    "    StructField(\"l_shipmode\", StringType()),\n",
    "    StructField(\"l_comment\", StringType())\n",
    "])\n",
    "\n",
    "NATION_SCHEMA = StructType([\n",
    "    StructField(\"n_nationkey\", LongType()), \n",
    "    StructField(\"n_name\", StringType()),\n",
    "    StructField(\"n_regionkey\", LongType()),\n",
    "    StructField(\"n_comment\", StringType()),\n",
    "])\n",
    "\n",
    "ORDER_SCHEMA = StructType([\n",
    "    StructField(\"o_orderkey\", LongType()),\n",
    "    StructField(\"o_custkey\", LongType()),\n",
    "    StructField(\"o_orderstatus\", StringType()),\n",
    "    StructField(\"o_totalprice\", DoubleType()),\n",
    "    StructField(\"o_orderdate\", StringType()),\n",
    "    StructField(\"o_orderpriority\", StringType()),\n",
    "    StructField(\"o_clerk\", StringType()),\n",
    "    StructField(\"o_shippriority\", LongType()),\n",
    "    StructField(\"o_comment\", StringType())\n",
    "])\n",
    "\n",
    "PART_SCHEMA = StructType([\n",
    "    StructField(\"p_partkey\", LongType()),    \n",
    "    StructField(\"p_name\", StringType()),\n",
    "    StructField(\"p_mfgr\", StringType()),\n",
    "    StructField(\"p_brand\", StringType()),\n",
    "    StructField(\"p_type\", StringType()),\n",
    "    StructField(\"p_size\", LongType()),\n",
    "    StructField(\"p_container\", StringType()),\n",
    "    StructField(\"p_retailprice\", DoubleType()),\n",
    "    StructField(\"p_comment\", StringType()),\n",
    "])\n",
    "\n",
    "PARTSUPP_SCHEMA = StructType([\n",
    "    StructField(\"ps_partkey\", LongType()),\n",
    "    StructField(\"ps_suppkey\", LongType()),\n",
    "    StructField(\"ps_availqty\", LongType()),\n",
    "    StructField(\"ps_supplycost\", DoubleType()),\n",
    "    StructField(\"ps_comment\", StringType())\n",
    "])\n",
    "\n",
    "REGION_SCHEMA = StructType([\n",
    "    StructField(\"r_regionkey\", LongType()),   \n",
    "    StructField(\"r_name\", StringType()),\n",
    "    StructField(\"r_comment\", StringType()),  \n",
    "])\n",
    "\n",
    "SUPPLIER_SCHEMA = StructType([\n",
    "    StructField(\"s_suppkey\", LongType()),    \n",
    "    StructField(\"s_name\", StringType()),\n",
    "    StructField(\"s_address\", StringType()),\n",
    "    StructField(\"s_nationkey\", LongType()),\n",
    "    StructField(\"s_phone\", StringType()),\n",
    "    StructField(\"s_acctbal\", DoubleType()),\n",
    "    StructField(\"s_comment\", StringType())\n",
    "])\n",
    "\n",
    "TABLE_SCHEMA_MAP = {\n",
    "        \"customer\": CUSTOMER_SCHEMA,\n",
    "        \"lineitem\": LINEITEM_SCHEMA,\n",
    "        \"nation\": NATION_SCHEMA,\n",
    "        \"region\": REGION_SCHEMA,\n",
    "        \"orders\": ORDER_SCHEMA,\n",
    "        \"part\": PART_SCHEMA,\n",
    "        \"partsupp\": PARTSUPP_SCHEMA,\n",
    "        \"supplier\": SUPPLIER_SCHEMA,\n",
    "}\n",
    "\n",
    "CURRENT_FILE_PATH = os.path.dirname(os.getcwd())\n",
    "if \"training_data\" not in CURRENT_FILE_PATH:\n",
    "    CURRENT_FILE_PATH += \"/training_data\"\n",
    "print(CURRENT_FILE_PATH)\n",
    "    \n",
    "def getSystemInfo():\n",
    "    info={}\n",
    "    try:\n",
    "        info['platform']=platform.system()\n",
    "        info['platform-release']=platform.release()\n",
    "        info['platform-version']=platform.version()\n",
    "        info['architecture']=platform.machine()\n",
    "        info['num_cpus'] = os.cpu_count()\n",
    "        info['hostname']=socket.gethostname()\n",
    "        info['ip-address']=socket.gethostbyname(socket.gethostname())\n",
    "        info['mac-address']=':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info['processor']=platform.processor()\n",
    "        info['ram']=str(round(psutil.virtual_memory().total / (1024.0 **3)))+\" GB\"\n",
    "        info['total_storage']=str(round(psutil.disk_usage(CURRENT_FILE_PATH).total / (1024.0 **3)))+\" GB\"\n",
    "        info['free_storage']=str(round(psutil.disk_usage(CURRENT_FILE_PATH).free / (1024.0 **3)))+\" GB\"\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "    return info\n",
    "\n",
    "# read in 22 TPCH queries\n",
    "TPCH_QUERIES = {}\n",
    "for i in range(1, 23):\n",
    "    with open(f\"{CURRENT_FILE_PATH}/queries/{i}.sql\") as f:\n",
    "        TPCH_QUERIES[i] = f.read() \n",
    "    \n",
    "def get_stats(runtimes):\n",
    "    print(f'median: {round(np.median(runtimes), 5)}, average: {round(np.average(runtimes), 5)}, std: {round(np.std(runtimes), 5)}, min: {round(min(runtimes), 5)}, max: {round(max(runtimes), 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c46ed8-e3d0-4851-ad65-52338c1dfb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries_og(parameters, n=10, debug=False, find_median_runtime=True):\n",
    "    '''\n",
    "    Run TPC-H queries 10 times and take the median runtime of each query \n",
    "    to generate a single training run for a set of parameters.\n",
    "    \n",
    "    Input: \n",
    "    parameters: list of parameter dictionaries \n",
    "    debug: if true will print out params and result time, false suppresses print statements\n",
    "    \n",
    "    Returns: \n",
    "    training_data dictionary with params and results \n",
    "    '''\n",
    "    result = {'params': [p.copy() for p in parameters], 'runtimes': {'total': []}}\n",
    "    spark = None\n",
    "    # add chosen parameter values to spark\n",
    "    param_name_index = {}\n",
    "    try:\n",
    "        conf = SparkConf(loadDefaults=False)\n",
    "        spark_params = []\n",
    "        for i, param in enumerate(parameters):\n",
    "            if param['spark_param']:\n",
    "                spark_params.append((param['name'], str(param['cur_value'])))\n",
    "                param_name_index[param['name']] = i\n",
    "\n",
    "        conf.setAll(spark_params)\n",
    "        spark  = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "                \n",
    "    except Exception as e:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        # this might happen because some parameters are related,\n",
    "        # and we might have made an impossible parameter assignment\n",
    "        result = {'params':parameters, 'runtimes': {}, 'msg': str(e)}\n",
    "        if debug:\n",
    "            print(\"error when setting \", parameters, e)\n",
    "        return result\n",
    "    \n",
    "    configurations = spark.sparkContext.getConf().getAll()\n",
    "    if debug:\n",
    "        print(\"Configuration\")\n",
    "    for item in configurations: \n",
    "        if debug:\n",
    "            print(item)\n",
    "        if param_name_index.get(item[0]) is not None:\n",
    "            assert item[1] == param[param_name_index.get(item[0])]['cur_value'], f'Spark session param {item} != {param[param_name_index.get(item[0])]}'\n",
    "    \n",
    "    # load tables\n",
    "    if debug:\n",
    "        print(\"loading tables\")\n",
    "    tables = {}\n",
    "    for table_name, table_schema in TABLE_SCHEMA_MAP.items():\n",
    "        table = spark.read.csv(f\"{CURRENT_FILE_PATH}/{SF_STR}/{table_name}.tbl\", sep = \"|\",\n",
    "                               schema=table_schema)\n",
    "        table.createOrReplaceTempView(table_name)\n",
    "        tables[table_name] = table\n",
    "    \n",
    "    if debug:\n",
    "        print(\"running queries\")\n",
    "    # take median of n runs for each query\n",
    "    for j in range(n):\n",
    "        result['runtimes']['total'].append(0)\n",
    "        for qnum, qtext in TPCH_QUERIES.items(): \n",
    "                # Measure execution time of sql query.\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                results = spark.sql(qtext, **tables)\n",
    "                end_time = time.time()\n",
    "                query_time = end_time - start_time\n",
    "                result['runtimes'].setdefault(qnum, []).append(query_time)\n",
    "                result['runtimes']['total'][-1] += query_time\n",
    "            except:\n",
    "                if debug:\n",
    "                    print(f\"failed while running query {qnum}...  \")\n",
    "    if debug:\n",
    "        print(\"done running queries\")\n",
    "    if find_median_runtime:\n",
    "        # take median of all runtimes as final output\n",
    "        for key, times in result['runtimes'].items():\n",
    "            result['runtimes'][key] = np.median(times)\n",
    "            if debug:\n",
    "                print(key, result['runtimes'][key])\n",
    "    # reset spark so we can load new param config next time\n",
    "    spark.stop()\n",
    "    #spark.newSession()#_instantiatedContext attribute of the session to None after calling session.stop().\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e7f15a-467b-423a-898a-63ba81fd7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = 1 # GB, default table scale factor\n",
    "job_name = 'local_run'\n",
    "SF_STR = f\"sf{sf}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42b3d5c-e1ae-4f87-8828-1b8eb4c37180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.22342, average: 0.22749, std: 0.03279, min: 0.19532, max: 0.33538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22656512260437012"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_queries_og([], n=100, find_median_runtime=False)\n",
    "get_stats(result['runtimes']['total'])\n",
    "result['runtimes']['total'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67529851-6fd6-4be5-b865-96be368fa23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(parameters, n=10, find_median_runtime=True):\n",
    "    '''\n",
    "    Run TPC-H queries 10 times and take the median runtime of each query \n",
    "    to generate a single training run for a set of parameters.\n",
    "    \n",
    "    Input: \n",
    "    parameters: list of parameter dictionaries \n",
    "    n: int number of times to run queries\n",
    "    find_median_runtime: if True returns runtimes values as a float (median), otherwise as a list of all n runtimes \n",
    "    \n",
    "    Returns: \n",
    "    training_data dictionary with params and results \n",
    "    '''\n",
    "    result = {'params': [p.copy() for p in parameters], 'runtimes': {'total': []}}\n",
    "    spark = None\n",
    "    # add chosen parameter values to spark\n",
    "    param_name_index = {}\n",
    "    try:\n",
    "        conf = SparkConf(loadDefaults=False)\n",
    "        spark_params = []\n",
    "        for i, param in enumerate(parameters):\n",
    "            if param['spark_param']:\n",
    "                spark_params.append((param['name'], str(param['cur_value'])))\n",
    "                param_name_index[param['name']] = i\n",
    "\n",
    "        conf.setAll(spark_params)\n",
    "        spark  = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "        spark.catalog.clearCache() # clear cache\n",
    "                \n",
    "    except Exception as e:\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "        # this might happen because some parameters are related,\n",
    "        # and we might have made an impossible parameter assignment\n",
    "        result = {'params':parameters, 'runtimes': {}, 'msg': str(e)}\n",
    "        return result\n",
    "    \n",
    "    configurations = spark.sparkContext.getConf().getAll()\n",
    "    for item in configurations: \n",
    "        if param_name_index.get(item[0]) is not None:\n",
    "            assert item[1] == param[param_name_index.get(item[0])]['cur_value'], f'Spark session param {item} != {param[param_name_index.get(item[0])]}'\n",
    "    \n",
    "    # load tables\n",
    "    tables = {}\n",
    "    for table_name, table_schema in TABLE_SCHEMA_MAP.items():\n",
    "        table = spark.read.csv(f\"{CURRENT_FILE_PATH}/{SF_STR}/{table_name}.tbl\", sep = \"|\",\n",
    "                               schema=table_schema)\n",
    "        table.createOrReplaceTempView(table_name)\n",
    "        tables[table_name] = table\n",
    "\n",
    "    # take median of n runs for each query\n",
    "    for j in range(n):\n",
    "        spark.catalog.clearCache() # clear cache before each run\n",
    "        result['runtimes']['total'].append(0)\n",
    "        for qnum, qtext in TPCH_QUERIES.items(): \n",
    "            # Measure execution time of sql query.\n",
    "            start_time = time.time()\n",
    "            results = spark.sql(qtext, **tables)\n",
    "            end_time = time.time()\n",
    "            query_time = end_time - start_time\n",
    "            result['runtimes'].setdefault(qnum, []).append(query_time)\n",
    "            result['runtimes']['total'][-1] += query_time\n",
    "\n",
    "    if find_median_runtime:\n",
    "        # take median of all runtimes as final output\n",
    "        for key, times in result['runtimes'].items():\n",
    "            result['runtimes'][key] = np.median(times)\n",
    "\n",
    "    # reset spark so we can load new param config next time\n",
    "    spark.catalog.clearCache() # clear cache at the end of each run just in case?\n",
    "    spark.stop()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8726ab3-5211-46e8-97ad-7a5bb0b7df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.22573, average: 0.23069, std: 0.02175, min: 0.1987, max: 0.30497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2225637435913086"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = run_queries([], n=100, find_median_runtime=False)\n",
    "get_stats(result['runtimes']['total'])\n",
    "result['runtimes']['total'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e0377e-4492-47e5-a0b2-47b819802d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.24389, average: 0.24441, std: 0.02324, min: 0.21301, max: 0.30499\n",
      "median: 0.22808, average: 0.23871, std: 0.02832, min: 0.21113, max: 0.39683\n",
      "---\n",
      "median: 0.22931, average: 0.24472, std: 0.03999, min: 0.21149, max: 0.37904\n",
      "median: 0.21289, average: 0.2282, std: 0.02711, min: 0.21152, max: 0.31709\n",
      "median: 0.21261, average: 0.22596, std: 0.02791, min: 0.21176, max: 0.31776\n",
      "median: 0.22027, average: 0.24046, std: 0.03544, min: 0.21211, max: 0.36166\n",
      "median: 0.23532, average: 0.25024, std: 0.04423, min: 0.21176, max: 0.39683\n",
      "median: 0.22195, average: 0.24089, std: 0.0348, min: 0.21165, max: 0.33474\n",
      "median: 0.21418, average: 0.23726, std: 0.03811, min: 0.21195, max: 0.35694\n",
      "median: 0.22072, average: 0.24038, std: 0.03516, min: 0.2121, max: 0.33191\n",
      "median: 0.22013, average: 0.24205, std: 0.03588, min: 0.21151, max: 0.35942\n",
      "median: 0.21381, average: 0.23838, std: 0.03429, min: 0.21165, max: 0.33048\n",
      "median: 0.23081, average: 0.24055, std: 0.02998, min: 0.2119, max: 0.31352\n",
      "median: 0.21405, average: 0.23467, std: 0.02991, min: 0.21155, max: 0.30858\n",
      "median: 0.23644, average: 0.24868, std: 0.04086, min: 0.21135, max: 0.36645\n",
      "median: 0.24997, average: 0.24746, std: 0.03334, min: 0.21114, max: 0.31588\n",
      "median: 0.21556, average: 0.23818, std: 0.03371, min: 0.21189, max: 0.31725\n",
      "median: 0.2193, average: 0.23107, std: 0.02152, min: 0.21229, max: 0.28122\n",
      "median: 0.22515, average: 0.23434, std: 0.02182, min: 0.21476, max: 0.32333\n",
      "median: 0.23432, average: 0.23642, std: 0.01591, min: 0.21572, max: 0.30674\n",
      "median: 0.22931, average: 0.23721, std: 0.02391, min: 0.21611, max: 0.32673\n",
      "median: 0.22324, average: 0.22819, std: 0.0159, min: 0.21155, max: 0.29193\n",
      "median: 0.21794, average: 0.22656, std: 0.02617, min: 0.21134, max: 0.34826\n",
      "median: 0.21514, average: 0.22893, std: 0.02388, min: 0.21168, max: 0.29971\n",
      "median: 0.22346, average: 0.23774, std: 0.02527, min: 0.22049, max: 0.32262\n",
      "median: 0.22399, average: 0.23801, std: 0.02552, min: 0.22071, max: 0.31637\n",
      "median: 0.22945, average: 0.24069, std: 0.02636, min: 0.22058, max: 0.31332\n",
      "median: 0.22746, average: 0.23605, std: 0.01862, min: 0.22093, max: 0.30147\n",
      "median: 0.24684, average: 0.25131, std: 0.02209, min: 0.22047, max: 0.31663\n",
      "median: 0.22508, average: 0.23291, std: 0.02049, min: 0.21976, max: 0.3284\n",
      "median: 0.22352, average: 0.23756, std: 0.02528, min: 0.22001, max: 0.32098\n",
      "median: 0.24516, average: 0.2502, std: 0.02971, min: 0.22108, max: 0.33941\n",
      "median: 0.2224, average: 0.23342, std: 0.02426, min: 0.22055, max: 0.34423\n",
      "median: 0.24494, average: 0.24313, std: 0.0184, min: 0.22136, max: 0.28972\n",
      "median: 0.2279, average: 0.2367, std: 0.02026, min: 0.2187, max: 0.31187\n",
      "median: 0.22359, average: 0.23294, std: 0.01947, min: 0.22121, max: 0.32433\n",
      "median: 0.2235, average: 0.2324, std: 0.01767, min: 0.22066, max: 0.29611\n",
      "median: 0.24473, average: 0.24444, std: 0.02724, min: 0.2212, max: 0.32634\n",
      "median: 0.2224, average: 0.23061, std: 0.01925, min: 0.2205, max: 0.31739\n",
      "median: 0.22174, average: 0.23194, std: 0.02044, min: 0.21956, max: 0.30788\n",
      "median: 0.24637, average: 0.24822, std: 0.02465, min: 0.22064, max: 0.3389\n",
      "median: 0.22732, average: 0.24065, std: 0.02624, min: 0.21691, max: 0.32357\n",
      "median: 0.23626, average: 0.24393, std: 0.02723, min: 0.22028, max: 0.34726\n",
      "median: 0.23126, average: 0.23941, std: 0.02158, min: 0.22059, max: 0.32053\n",
      "median: 0.24099, average: 0.24966, std: 0.02109, min: 0.22887, max: 0.30751\n",
      "median: 0.23872, average: 0.24824, std: 0.02337, min: 0.22047, max: 0.30978\n",
      "median: 0.23629, average: 0.2445, std: 0.02347, min: 0.21987, max: 0.30791\n",
      "median: 0.22574, average: 0.23481, std: 0.02594, min: 0.21858, max: 0.33316\n",
      "median: 0.24518, average: 0.24586, std: 0.02316, min: 0.22096, max: 0.32457\n",
      "median: 0.24085, average: 0.24221, std: 0.02715, min: 0.21137, max: 0.30625\n",
      "median: 0.21232, average: 0.22757, std: 0.02444, min: 0.21113, max: 0.29171\n",
      "median: 0.21853, average: 0.23975, std: 0.03523, min: 0.21129, max: 0.3279\n"
     ]
    }
   ],
   "source": [
    "results_combo = []\n",
    "for i in range(50):\n",
    "    result = run_queries_og([], n=50, find_median_runtime=False)\n",
    "    results_combo.append(result['runtimes']['total'])\n",
    "first_times = [r[0] for r in results_combo]\n",
    "all_times = []\n",
    "for r in results_combo:\n",
    "    all_times += r\n",
    "\n",
    "get_stats(first_times)\n",
    "get_stats(all_times)\n",
    "print('---')\n",
    "for r in results_combo:\n",
    "    get_stats(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "120e6894-0ac3-4f09-b36f-251f72586797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.2297, average: 0.24443, std: 0.03928, min: 0.21297, max: 0.35476\n",
      "median: 0.21946, average: 0.24196, std: 0.03845, min: 0.21118, max: 0.74749\n",
      "---\n",
      "median: 0.2194, average: 0.23847, std: 0.03307, min: 0.21127, max: 0.3328\n",
      "median: 0.23846, average: 0.25215, std: 0.0426, min: 0.2116, max: 0.35511\n",
      "median: 0.22522, average: 0.24008, std: 0.03174, min: 0.2117, max: 0.30977\n",
      "median: 0.21252, average: 0.22479, std: 0.02312, min: 0.21153, max: 0.29154\n",
      "median: 0.21275, average: 0.24441, std: 0.08209, min: 0.21151, max: 0.74749\n",
      "median: 0.21368, average: 0.23668, std: 0.04049, min: 0.21138, max: 0.4195\n",
      "median: 0.24035, average: 0.24102, std: 0.02753, min: 0.21179, max: 0.30555\n",
      "median: 0.22158, average: 0.24681, std: 0.04656, min: 0.212, max: 0.4189\n",
      "median: 0.21265, average: 0.22796, std: 0.02804, min: 0.21147, max: 0.32695\n",
      "median: 0.24069, average: 0.24752, std: 0.03746, min: 0.21167, max: 0.34528\n",
      "median: 0.24941, average: 0.25324, std: 0.03403, min: 0.21129, max: 0.33607\n",
      "median: 0.21306, average: 0.22879, std: 0.0322, min: 0.21191, max: 0.37089\n",
      "median: 0.22087, average: 0.24751, std: 0.04146, min: 0.21139, max: 0.34856\n",
      "median: 0.23843, average: 0.24673, std: 0.03793, min: 0.21151, max: 0.36191\n",
      "median: 0.24369, average: 0.25069, std: 0.03725, min: 0.21153, max: 0.37237\n",
      "median: 0.21365, average: 0.2365, std: 0.03423, min: 0.21169, max: 0.33796\n",
      "median: 0.23137, average: 0.24402, std: 0.03259, min: 0.21177, max: 0.32946\n",
      "median: 0.21733, average: 0.24052, std: 0.0394, min: 0.21118, max: 0.38156\n",
      "median: 0.24698, average: 0.25259, std: 0.04122, min: 0.21135, max: 0.36038\n",
      "median: 0.21382, average: 0.23818, std: 0.03825, min: 0.2115, max: 0.39684\n",
      "median: 0.23379, average: 0.2479, std: 0.03737, min: 0.21171, max: 0.34747\n",
      "median: 0.2316, average: 0.24562, std: 0.03686, min: 0.21164, max: 0.3231\n",
      "median: 0.22395, average: 0.2435, std: 0.03441, min: 0.21133, max: 0.31659\n",
      "median: 0.23147, average: 0.24355, std: 0.03337, min: 0.2117, max: 0.35258\n",
      "median: 0.21561, average: 0.24221, std: 0.04143, min: 0.21204, max: 0.34528\n",
      "median: 0.23663, average: 0.24465, std: 0.02992, min: 0.21204, max: 0.32071\n",
      "median: 0.23251, average: 0.24697, std: 0.03636, min: 0.21203, max: 0.34706\n",
      "median: 0.2209, average: 0.24179, std: 0.03269, min: 0.21178, max: 0.31275\n",
      "median: 0.21721, average: 0.24505, std: 0.04103, min: 0.21164, max: 0.37562\n",
      "median: 0.22446, average: 0.24568, std: 0.03934, min: 0.21156, max: 0.33993\n",
      "median: 0.22674, average: 0.24472, std: 0.03913, min: 0.2118, max: 0.37834\n",
      "median: 0.21719, average: 0.23198, std: 0.02861, min: 0.21177, max: 0.3113\n",
      "median: 0.21614, average: 0.23745, std: 0.03302, min: 0.21159, max: 0.31924\n",
      "median: 0.21769, average: 0.24535, std: 0.04145, min: 0.21158, max: 0.33687\n",
      "median: 0.22363, average: 0.24356, std: 0.03562, min: 0.21201, max: 0.33131\n",
      "median: 0.21249, average: 0.23341, std: 0.03782, min: 0.21131, max: 0.36705\n",
      "median: 0.21268, average: 0.23813, std: 0.03704, min: 0.21123, max: 0.34378\n",
      "median: 0.21248, average: 0.22907, std: 0.03511, min: 0.2113, max: 0.35396\n",
      "median: 0.2132, average: 0.23468, std: 0.03594, min: 0.21193, max: 0.34837\n",
      "median: 0.21313, average: 0.23925, std: 0.03861, min: 0.21165, max: 0.37683\n",
      "median: 0.2187, average: 0.23465, std: 0.02681, min: 0.21176, max: 0.30242\n",
      "median: 0.21952, average: 0.24027, std: 0.03724, min: 0.21161, max: 0.34709\n",
      "median: 0.21896, average: 0.24025, std: 0.03401, min: 0.21189, max: 0.32238\n",
      "median: 0.24751, average: 0.25172, std: 0.03818, min: 0.21143, max: 0.3522\n",
      "median: 0.21267, average: 0.23633, std: 0.03721, min: 0.21152, max: 0.34674\n",
      "median: 0.21321, average: 0.23934, std: 0.03629, min: 0.21172, max: 0.3125\n",
      "median: 0.25228, average: 0.25297, std: 0.03755, min: 0.21134, max: 0.33016\n",
      "median: 0.23775, average: 0.25249, std: 0.0406, min: 0.21171, max: 0.35503\n",
      "median: 0.21994, average: 0.23714, std: 0.03256, min: 0.21185, max: 0.3497\n",
      "median: 0.22648, average: 0.24983, std: 0.04287, min: 0.21195, max: 0.37303\n"
     ]
    }
   ],
   "source": [
    "results_combo = []\n",
    "for i in range(50):\n",
    "    result = run_queries([], n=50, find_median_runtime=False)\n",
    "    results_combo.append(result['runtimes']['total'])\n",
    "first_times = [r[0] for r in results_combo]\n",
    "all_times = []\n",
    "for r in results_combo:\n",
    "    all_times += r\n",
    "\n",
    "get_stats(first_times)\n",
    "get_stats(all_times)\n",
    "print('---')\n",
    "for r in results_combo:\n",
    "    get_stats(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d50ba3c-f2f4-4c4d-9826-0ebe71957ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "931edd2f-9fd6-4305-9320-6616fc255a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hoped/spark-autotuner/training_data/../TPC-H V3.0.1/dbgen'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TABLE_FILE_PATH = CURRENT_FILE_PATH + \"/../TPC-H V3.0.1/dbgen\"\n",
    "TABLE_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1701d678-eed9-4541-91b9-629681a43acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['supplier.tbl',\n",
       "  'customer.tbl',\n",
       "  'orders.tbl',\n",
       "  'lineitem.tbl',\n",
       "  'part.tbl',\n",
       "  'partsupp.tbl',\n",
       "  'nation.tbl',\n",
       "  'region.tbl'],\n",
       " 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direc = TABLE_FILE_PATH\n",
    "files = os.listdir(direc)\n",
    "files = [f for f in files if os.path.isfile(direc+'/'+f) and '.tbl' in f] #just files\n",
    "table_sizes = [os.path.getsize(f'{TABLE_FILE_PATH}/{f}') for f in files]\n",
    "print(round(sum(table_sizes)/ (1024.0 **3)))\n",
    "num_files = len(files)\n",
    "\n",
    "files, num_files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c730599-2580-4be6-a8aa-f8530b12f047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
